{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import swyft\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import stats\n",
    "import seaborn as sbn\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.integrate import trapezoid\n",
    "from scipy.interpolate import CloughTocher2DInterpolator\n",
    "from scipy.integrate import simps\n",
    "from matplotlib.pyplot import contour, show\n",
    "from matplotlib.lines import Line2D\n",
    "import emcee\n",
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchist\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "pallete = np.flip(sns.color_palette(\"tab20c\", 8), axis = 0)\n",
    "cross_sec_th = -49\n",
    "\n",
    "long_planck = 1.616199 * 1e-35 * 1e2 # cm\n",
    "masa_planck = 2.435 * 1e18 # GeV\n",
    "fac = (long_planck * masa_planck) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "# Email configuration\n",
    "sender_email = 'martindelosrios13@gmail.com'\n",
    "app_password = 'ukgl cvyy glqk woki'  # Use the app password you generated\n",
    "recipient_email = 'martindelosrios13@gmail.com'\n",
    "subject = 'Termino'\n",
    "message = 'Termino de correr'\n",
    "\n",
    "# Connect to the SMTP server\n",
    "smtp_server = 'smtp.gmail.com'\n",
    "smtp_port = 587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1b323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from playsound import playsound\n",
    "#playsound('/home/martinrios/Downloads/mario.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e43e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40927498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is usefull to print the versions of the package that we are using\n",
    "print('swyft version:', swyft.__version__)\n",
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', mpl.__version__)\n",
    "print('torch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_rate = \"#d55e00\"\n",
    "color_drate = 'darkblue' #\"#0072b2\"\n",
    "color_s1s2 = 'limegreen' #\"#009e73\"\n",
    "color_comb = 'limegreen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1535586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    device = 'gpu'\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b156a22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_slice(datFolder):\n",
    "    nobs_slices = 0\n",
    "    for i, folder in enumerate(datFolder):\n",
    "        print('Reading data from ' + folder)\n",
    "        if i == 0:\n",
    "            pars_slices      = np.loadtxt(folder + 'pars.txt') # pars[:,0] = mass ; pars[:,1] = cross-section ; pars[:,2] = theta\n",
    "            rate_raw_slices  = np.loadtxt(folder + 'rate.txt') # rate[:,0] = total expected events ; rate[:,1] = expected signal ; rate[:,2] = # events pseudo-experiment ; rate[:,3] = # signal events pseudo-experiment \n",
    "            \n",
    "            diff_rate_WIMP     = np.loadtxt(folder + 'diff_rate_WIMP.txt')\n",
    "            diff_rate_er       = np.loadtxt(folder + 'diff_rate_er.txt')\n",
    "            diff_rate_ac       = np.loadtxt(folder + 'diff_rate_ac.txt')\n",
    "            diff_rate_cevns_SM = np.loadtxt(folder + 'diff_rate_CEVNS-SM.txt')\n",
    "            diff_rate_radio    = np.loadtxt(folder + 'diff_rate_radiogenics.txt')\n",
    "            diff_rate_wall     = np.loadtxt(folder + 'diff_rate_wall.txt')\n",
    "            \n",
    "            s1s2_WIMP_slices     = np.loadtxt(folder + 's1s2_WIMP.txt')\n",
    "            s1s2_er_slices       = np.loadtxt(folder + 's1s2_er.txt')\n",
    "            s1s2_ac_slices       = np.loadtxt(folder + 's1s2_ac.txt')\n",
    "            s1s2_cevns_SM_slices = np.loadtxt(folder + 's1s2_CEVNS-SM.txt')\n",
    "            s1s2_radio_slices    = np.loadtxt(folder + 's1s2_radiogenics.txt')\n",
    "            s1s2_wall_slices     = np.loadtxt(folder + 's1s2_wall.txt')\n",
    "        else:\n",
    "            pars_slices      = np.vstack((pars_slices, np.loadtxt(folder + 'pars.txt'))) # pars[:,0] = mass ; pars[:,1] = cross-section ; pars[:,2] = theta\n",
    "            rate_raw_slices  = np.vstack((rate_raw_slices, np.loadtxt(folder + 'rate.txt'))) # rate[:,0] = total expected events ; rate[:,1] = expected signal ; rate[:,2] = # events pseudo-experiment ; rate[:,3] = # signal events pseudo-experiment \n",
    "            \n",
    "            diff_rate_WIMP     = np.vstack((diff_rate_WIMP, np.loadtxt(folder + 'diff_rate_WIMP.txt') ))\n",
    "            diff_rate_er       = np.vstack((diff_rate_er, np.loadtxt(folder + 'diff_rate_er.txt') ))\n",
    "            diff_rate_ac       = np.vstack((diff_rate_ac, np.loadtxt(folder + 'diff_rate_ac.txt') ))\n",
    "            diff_rate_cevns_SM = np.vstack((diff_rate_cevns_SM, np.loadtxt(folder + 'diff_rate_CEVNS-SM.txt') ))\n",
    "            diff_rate_radio    = np.vstack((diff_rate_radio, np.loadtxt(folder + 'diff_rate_radiogenics.txt') ))\n",
    "            diff_rate_wall     = np.vstack((diff_rate_wall, np.loadtxt(folder + 'diff_rate_wall.txt') ))\n",
    "            \n",
    "            s1s2_WIMP_slices     = np.vstack((s1s2_WIMP_slices, np.loadtxt(folder + 's1s2_WIMP.txt')))\n",
    "            s1s2_er_slices       = np.vstack((s1s2_er_slices, np.loadtxt(folder + 's1s2_er.txt')))\n",
    "            s1s2_ac_slices       = np.vstack((s1s2_ac_slices, np.loadtxt(folder + 's1s2_ac.txt')))\n",
    "            s1s2_cevns_SM_slices = np.vstack((s1s2_cevns_SM_slices, np.loadtxt(folder + 's1s2_CEVNS-SM.txt')))\n",
    "            s1s2_radio_slices    = np.vstack((s1s2_radio_slices, np.loadtxt(folder + 's1s2_radiogenics.txt')))\n",
    "            s1s2_wall_slices     = np.vstack((s1s2_wall_slices, np.loadtxt(folder + 's1s2_wall.txt')))\n",
    "            \n",
    "        \n",
    "    nobs_slices = len(pars_slices) # Total number of observations\n",
    "    print('We have ' + str(nobs_slices) + ' observations...')\n",
    "    \n",
    "    s1s2_slices = s1s2_WIMP_slices + s1s2_er_slices + s1s2_ac_slices + s1s2_cevns_SM_slices + s1s2_radio_slices + s1s2_wall_slices\n",
    "    rate_slices = np.sum(s1s2_slices, axis = 1) # Just to have the same as on the other notebooks. This already includes the backgrounds\n",
    "    s1s2_slices = s1s2_slices.reshape(nobs_slices, 97, 97)\n",
    "\n",
    "    diff_rate_slices = diff_rate_WIMP + diff_rate_er + diff_rate_ac + diff_rate_cevns_SM + diff_rate_radio + diff_rate_wall\n",
    "    \n",
    "    # Let's work with the log of the mass and cross-section\n",
    "    \n",
    "    pars_slices[:,0] = np.log10(pars_slices[:,0])\n",
    "    pars_slices[:,1] = np.log10(pars_slices[:,1])\n",
    "    \n",
    "    return pars_slices, rate_slices, diff_rate_slices, s1s2_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7302c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1dpost(x, h1, ax, low_1sigma = None, up_1sigma = None, alpha = 1, color = 'black', real_val = True):\n",
    "    ax.plot(x, h1, c = color, alpha = alpha)\n",
    "    if real_val: ax.axvline(x = pars_true[1], c = 'orange')\n",
    "    ax.axvline(x = -49, c = 'black', linewidth = 2)\n",
    "\n",
    "    if (low_1sigma is not None) & (up_1sigma is not None):\n",
    "        ax.axvline(low_1sigma, c = 'black', linestyle = '--')\n",
    "        ax.axvline(up_1sigma, c = 'black', linestyle = '--')\n",
    "    \n",
    "    #ax.axvline(low_2sigma, c = 'black', linestyle = '--')\n",
    "    #ax.axvline(up_2sigma, c = 'black', linestyle = '--')\n",
    "    \n",
    "    #ax.axvline(low_3sigma, c = 'black', linestyle = ':')\n",
    "    #ax.axvline(up_3sigma, c = 'black', linestyle = ':')\n",
    "\n",
    "    ax.set_xlim(-50, -43)\n",
    "    #ax.xscale('log')\n",
    "    ax.set_xlabel('$log(\\sigma)$')\n",
    "    ax.set_ylabel('$P(\\sigma|x)$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9293d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email(message = 'termino'):\n",
    "    # Create a MIMEText object to represent the email message\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "    msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "    server = smtplib.SMTP(smtp_server, smtp_port)\n",
    "    server.starttls()\n",
    "    # Log in to your email account with the app password\n",
    "    server.login(sender_email, app_password)\n",
    "    \n",
    "    # Send the email\n",
    "    server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "    \n",
    "    # Close the connection\n",
    "    server.quit()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot1d(ax, predictions, pars_true, par = 1, \n",
    "           xlabel = '$\\log_{10}(\\sigma)$', ylabel = '$P(\\sigma|x)\\ /\\ P(\\sigma)$',\n",
    "           flip = False, fill = True, linestyle = 'solid', color = 'black', fac = 1):\n",
    "    # Let's put the results in arrays\n",
    "    parameter = np.asarray(predictions[0].params[:,par,0]) * (pars_max[par] - pars_min[par]) + pars_min[par]\n",
    "    ratios = np.exp(np.asarray(predictions[0].logratios[:,par]))\n",
    "    \n",
    "    ind_sort  = np.argsort(parameter)\n",
    "    ratios    = ratios[ind_sort]\n",
    "    parameter = parameter[ind_sort]\n",
    "    \n",
    "    # Let's compute the integrated probability for different threshold\n",
    "    cuts = np.linspace(np.min(ratios), np.max(ratios), 100)\n",
    "    integrals = []\n",
    "    for c in cuts:\n",
    "        ratios0 = np.copy(ratios)\n",
    "        ratios0[np.where(ratios < c)[0]] = 0 \n",
    "        integrals.append( trapezoid(ratios0, parameter) / trapezoid(ratios, parameter) )\n",
    "        \n",
    "    integrals = np.asarray(integrals)\n",
    "    \n",
    "    # Let's compute the thresholds corresponding to 0.9 and 0.95 integrated prob\n",
    "    cut90 = cuts[np.argmin( np.abs(integrals - 0.9))]\n",
    "    cut95 = cuts[np.argmin( np.abs(integrals - 0.95))]\n",
    "\n",
    "    if not flip:\n",
    "        ax.plot(10**parameter, fac * ratios, c = color, linestyle = linestyle)\n",
    "        if fill:\n",
    "            ind = np.where(ratios > cut90)[0]\n",
    "            ax.fill_between(10**parameter[ind], fac * ratios[ind], [0] * len(ind), color = 'darkcyan', alpha = 0.3)\n",
    "            ind = np.where(ratios > cut95)[0]\n",
    "            ax.fill_between(10**parameter[ind], fac * ratios[ind], [0] * len(ind), color = 'darkcyan', alpha = 0.5)\n",
    "        ax.axvline(x = 10**(pars_true[par] * (pars_max[par] - pars_min[par]) + pars_min[par]), color = 'black')\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_xscale('log')\n",
    "    else:\n",
    "        ax.plot(fac * ratios, 10**parameter, c = color, linestyle = linestyle)\n",
    "        if fill:\n",
    "            ind = np.where(ratios > cut90)[0]\n",
    "            ax.fill_betweenx(10**parameter[ind], [0] * len(ind), fac * ratios[ind], color = 'darkcyan', alpha = 0.3)\n",
    "            ind = np.where(ratios > cut95)[0]\n",
    "            ax.fill_betweenx(10**parameter[ind], [0] * len(ind), fac * ratios[ind], color = 'darkcyan', alpha = 0.5) \n",
    "        ax.axhline(y = 10**(pars_true[par] * (pars_max[par] - pars_min[par]) + pars_min[par]), color = 'black')\n",
    "        ax.set_xlabel(ylabel)\n",
    "        ax.set_ylabel(xlabel)\n",
    "        #ax.set_xlim(-0.1,8)\n",
    "        ax.set_ylim(1e-50, 1e-42)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2d(ax, predictions, pars_true, fill = True, line = False, linestyle = 'solid', color = 'black'):      \n",
    "    results_pars = np.asarray(predictions[1].params)\n",
    "    results      = np.asarray(predictions[1].logratios)\n",
    "    \n",
    "    # Let's make an interpolation function \n",
    "    interp = CloughTocher2DInterpolator(results_pars[:,0,:], np.exp(results[:,0]))\n",
    "    \n",
    "    def interpol(log_m, log_sigma):\n",
    "        m_norm = (log_m - pars_min[0]) / (pars_max[0] - pars_min[0])\n",
    "        sigma_norm = (log_sigma - pars_min[1]) / (pars_max[1] - pars_min[1])\n",
    "        return interp(m_norm, sigma_norm)\n",
    "        \n",
    "    # Let's estimate the value of the posterior in a grid\n",
    "    nvals = 20\n",
    "    m_values = np.logspace(0.8, 2.99, nvals)\n",
    "    s_values = np.logspace(-49., -43.1, nvals)\n",
    "    m_grid, s_grid = np.meshgrid(m_values, s_values)\n",
    "    \n",
    "    ds = np.log10(s_values[1]) - np.log10(s_values[0])\n",
    "    dm = np.log10(m_values[1]) - np.log10(m_values[0])\n",
    "    \n",
    "    res = np.zeros((nvals, nvals))\n",
    "    for m in range(nvals):\n",
    "        for s in range(nvals):\n",
    "            res[m,s] = interpol(np.log10(m_values[m]), np.log10(s_values[s]))\n",
    "    res[np.isnan(res)] = 0\n",
    "    #print(res)\n",
    "    # Let's compute the integral\n",
    "    norm = simps(simps(res, dx=dm, axis=1), dx=ds)\n",
    "    #print(norm)\n",
    "    \n",
    "    # Let's look for the 0.9 probability threshold\n",
    "    cuts = np.linspace(np.min(res), np.max(res), 100)\n",
    "    integrals = []\n",
    "    for c in cuts:\n",
    "        res0 = np.copy(res)\n",
    "        res0[np.where(res < c)[0], np.where(res < c)[1]] = 0\n",
    "        integrals.append( simps(simps(res0, dx=dm, axis=1), dx=ds) / norm )\n",
    "    integrals = np.asarray(integrals)\n",
    "    \n",
    "    cut90 = cuts[np.argmin( np.abs(integrals - 0.9))]\n",
    "    cut95 = cuts[np.argmin( np.abs(integrals - 0.95))]\n",
    "    #print(cut)\n",
    "    if fill:\n",
    "        ax.contourf(m_values, s_values, res.T, levels = [0, cut90, np.max(res)], colors = ['white','darkcyan'], alpha = 0.3, linestyles = ['solid'])\n",
    "        ax.contourf(m_values, s_values, res.T, levels = [0, cut95, np.max(res)], colors = ['white','darkcyan'], alpha = 0.5, linestyles = ['solid'])\n",
    "    if line:\n",
    "        ax.contour(m_values, s_values, res.T, levels = [0,cut90], colors = [color], linestyles = ['solid'])\n",
    "        ax.contour(m_values, s_values, res.T, levels = [0,cut95], colors = [color], linestyles = ['--'])\n",
    "    \n",
    "    ax.axvline(x = 10**(pars_true[0] * (pars_max[0] - pars_min[0]) + pars_min[0]), color = 'black')\n",
    "    ax.axhline(y = 10**(pars_true[1] * (pars_max[1] - pars_min[1]) + pars_min[1]), color = 'black')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('$M_{DM}$ [GeV]')\n",
    "    ax.set_ylabel('$\\sigma$ $[cm^{2}]$')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot1d_comb(ax, predictions, pars_true, par = 1, \n",
    "           xlabel = '$\\log_{10}(\\sigma)$', ylabel = '$P(\\sigma|x)\\ /\\ P(\\sigma)$',\n",
    "           flip = False, fill = True, linestyle = 'solid', color = 'black', fac = 1):\n",
    "    # Let's put the results in arrays\n",
    "    parameter = np.asarray(predictions[0][0].params[:,par,0]) * (pars_max[par] - pars_min[par]) + pars_min[par]\n",
    "    ratios = np.zeros_like(predictions[0][0].logratios[:,par])\n",
    "    for pred in predictions:\n",
    "        ratios = ratios + np.asarray(pred[0].logratios[:,par])\n",
    "    ratios = np.exp(ratios)\n",
    "    \n",
    "    ind_sort  = np.argsort(parameter)\n",
    "    ratios    = ratios[ind_sort]\n",
    "    parameter = parameter[ind_sort]\n",
    "    \n",
    "    # Let's compute the integrated probability for different threshold\n",
    "    cuts = np.linspace(np.min(ratios), np.max(ratios), 100)\n",
    "    integrals = []\n",
    "    for c in cuts:\n",
    "        ratios0 = np.copy(ratios)\n",
    "        ratios0[np.where(ratios < c)[0]] = 0 \n",
    "        integrals.append( trapezoid(ratios0, parameter) / trapezoid(ratios, parameter) )\n",
    "        \n",
    "    integrals = np.asarray(integrals)\n",
    "    \n",
    "    # Let's compute the thresholds corresponding to 0.9 and 0.95 integrated prob\n",
    "    cut90 = cuts[np.argmin( np.abs(integrals - 0.9))]\n",
    "    cut95 = cuts[np.argmin( np.abs(integrals - 0.95))]\n",
    "\n",
    "    if not flip:\n",
    "        ax.plot(10**parameter, fac * ratios, c = color, linestyle = linestyle)\n",
    "        if fill:\n",
    "            ind = np.where(ratios > cut90)[0]\n",
    "            ax.fill_between(10**parameter[ind], fac * ratios[ind], [0] * len(ind), color = 'darkcyan', alpha = 0.3)\n",
    "            ind = np.where(ratios > cut95)[0]\n",
    "            ax.fill_between(10**parameter[ind], fac * ratios[ind], [0] * len(ind), color = 'darkcyan', alpha = 0.5)\n",
    "        ax.axvline(x = 10**(pars_true[par] * (pars_max[par] - pars_min[par]) + pars_min[par]), color = 'black')\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_xscale('log')\n",
    "    else:\n",
    "        ax.plot(fac * ratios, 10**parameter, c = color, linestyle = linestyle)\n",
    "        if fill:\n",
    "            ind = np.where(ratios > cut90)[0]\n",
    "            ax.fill_betweenx(10**parameter[ind], [0] * len(ind), fac * ratios[ind], color = 'darkcyan', alpha = 0.3)\n",
    "            ind = np.where(ratios > cut95)[0]\n",
    "            ax.fill_betweenx(10**parameter[ind], [0] * len(ind), fac * ratios[ind], color = 'darkcyan', alpha = 0.5) \n",
    "        ax.axhline(y = 10**(pars_true[par] * (pars_max[par] - pars_min[par]) + pars_min[par]), color = 'black')\n",
    "        ax.set_xlabel(ylabel)\n",
    "        ax.set_ylabel(xlabel)\n",
    "        #ax.set_xlim(-0.1,8)\n",
    "        ax.set_ylim(1e-50, 1e-42)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot2d_comb(ax, predictions, pars_true, fill = True, line = False, linestyle = 'solid', color = 'black'):    \n",
    "    \n",
    "    results_pars = np.asarray(predictions[0][1].params)\n",
    "    results = np.zeros_like(predictions[0][1].logratios)\n",
    "    for pred in predictions:\n",
    "        results = results + np.asarray(pred[1].logratios)\n",
    "    \n",
    "    # Let's make an interpolation function \n",
    "    interp = CloughTocher2DInterpolator(results_pars[:,0,:], np.exp(results[:,0]))\n",
    "    \n",
    "    def interpol(log_m, log_sigma):\n",
    "        m_norm = (log_m - pars_min[0]) / (pars_max[0] - pars_min[0])\n",
    "        sigma_norm = (log_sigma - pars_min[1]) / (pars_max[1] - pars_min[1])\n",
    "        return interp(m_norm, sigma_norm)\n",
    "        \n",
    "    # Let's estimate the value of the posterior in a grid\n",
    "    nvals = 20\n",
    "    m_values = np.logspace(0.8, 2.99, nvals)\n",
    "    s_values = np.logspace(-49., -43.1, nvals)\n",
    "    m_grid, s_grid = np.meshgrid(m_values, s_values)\n",
    "    \n",
    "    ds = np.log10(s_values[1]) - np.log10(s_values[0])\n",
    "    dm = np.log10(m_values[1]) - np.log10(m_values[0])\n",
    "    \n",
    "    res = np.zeros((nvals, nvals))\n",
    "    for m in range(nvals):\n",
    "        for s in range(nvals):\n",
    "            res[m,s] = interpol(np.log10(m_values[m]), np.log10(s_values[s]))\n",
    "    res[np.isnan(res)] = 0\n",
    "    # Let's compute the integral\n",
    "    norm = simps(simps(res, dx=dm, axis=1), dx=ds)\n",
    "    \n",
    "    # Let's look for the 0.9 probability threshold\n",
    "    cuts = np.linspace(np.min(res), np.max(res), 100)\n",
    "    integrals = []\n",
    "    for c in cuts:\n",
    "        res0 = np.copy(res)\n",
    "        res0[np.where(res < c)[0], np.where(res < c)[1]] = 0\n",
    "        integrals.append( simps(simps(res0, dx=dm, axis=1), dx=ds) / norm )\n",
    "    integrals = np.asarray(integrals)\n",
    "    \n",
    "    cut90 = cuts[np.argmin( np.abs(integrals - 0.9))]\n",
    "    cut95 = cuts[np.argmin( np.abs(integrals - 0.95))]\n",
    "    if fill:\n",
    "        ax.contourf(m_values, s_values, res.T, levels = [0, cut90, np.max(res)], colors = ['white','darkcyan'], alpha = 0.3, linestyles = ['solid'])\n",
    "        ax.contourf(m_values, s_values, res.T, levels = [0, cut95, np.max(res)], colors = ['white','darkcyan'], alpha = 0.5, linestyles = ['solid'])\n",
    "    if line:\n",
    "        ax.contour(m_values, s_values, res.T, levels = [0,cut90], colors = [color], linestyles = ['solid'])\n",
    "        ax.contour(m_values, s_values, res.T, levels = [0,cut95], colors = [color], linestyles = ['--'])\n",
    "    \n",
    "    ax.axvline(x = 10**(pars_true[0] * (pars_max[0] - pars_min[0]) + pars_min[0]), color = 'black')\n",
    "    ax.axhline(y = 10**(pars_true[1] * (pars_max[1] - pars_min[1]) + pars_min[1]), color = 'black')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('$M_{DM}$ [GeV]')\n",
    "    ax.set_ylabel('$\\sigma$ $[cm^{2}]$')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea041df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf033d27-44a4-4b9f-9f5f-53915b79d32e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "nobs_new = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9583add-fe04-4409-816a-07965ec91317",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    valLoss = np.load('O4_valLoss_nobs_' + str(nobs_new) + '.npy')\n",
    "    iter = len(valLoss[0,:])\n",
    "    valLoss = np.hstack((valLoss, np.zeros((3,1))))\n",
    "except:\n",
    "    valLoss = np.zeros((3,1))\n",
    "    iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baccbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/andresData/O4-fulldata/O4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where are your files?\n",
    "datFolder = ['../data/andresData/O4-fulldata/O4/O4-run01/',\n",
    "             '../data/andresData/O4-fulldata/O4/O4-run02/',\n",
    "             '../data/andresData/O4-fulldata/O4/O4-run03/',\n",
    "             #'../data/andresData/O4-fulldata/O4/O4-run04/'\n",
    "            ]\n",
    "nobs = 0\n",
    "for i, folder in enumerate(datFolder):\n",
    "    print(i)\n",
    "    if i == 0:\n",
    "        pars      = np.loadtxt(folder + 'pars.txt') # pars[:,0] = mass ; pars[:,1] = cross-section ; pars[:,2] = theta\n",
    "        rate_raw  = np.loadtxt(folder + 'rate.txt') # rate[:,0] = total expected events ; rate[:,1] = expected signal ; rate[:,2] = # events pseudo-experiment ; rate[:,3] = # signal events pseudo-experiment \n",
    "        \n",
    "        diff_rate_WIMP     = np.loadtxt(folder + 'diff_rate_WIMP.txt')\n",
    "        diff_rate_er       = np.loadtxt(folder + 'diff_rate_er.txt')\n",
    "        diff_rate_ac       = np.loadtxt(folder + 'diff_rate_ac.txt')\n",
    "        diff_rate_cevns_SM = np.loadtxt(folder + 'diff_rate_CEVNS-SM.txt')\n",
    "        diff_rate_radio    = np.loadtxt(folder + 'diff_rate_radiogenics.txt')\n",
    "        diff_rate_wall     = np.loadtxt(folder + 'diff_rate_wall.txt')\n",
    "        \n",
    "        s1s2_WIMP     = np.loadtxt(folder + 's1s2_WIMP.txt')\n",
    "        s1s2_er       = np.loadtxt(folder + 's1s2_er.txt')\n",
    "        s1s2_ac       = np.loadtxt(folder + 's1s2_ac.txt')\n",
    "        s1s2_cevns_SM = np.loadtxt(folder + 's1s2_CEVNS-SM.txt')\n",
    "        s1s2_radio    = np.loadtxt(folder + 's1s2_radiogenics.txt')\n",
    "        s1s2_wall     = np.loadtxt(folder + 's1s2_wall.txt')\n",
    "    else:\n",
    "        pars      = np.vstack((pars, np.loadtxt(folder + 'pars.txt'))) # pars[:,0] = mass ; pars[:,1] = cross-section ; pars[:,2] = theta\n",
    "        rate_raw  = np.vstack((rate_raw, np.loadtxt(folder + 'rate.txt'))) # rate[:,0] = total expected events ; rate[:,1] = expected signal ; rate[:,2] = # events pseudo-experiment ; rate[:,3] = # signal events pseudo-experiment \n",
    "        \n",
    "        diff_rate_WIMP     = np.vstack(( diff_rate_WIMP, np.loadtxt(folder + 'diff_rate_WIMP.txt')))\n",
    "        diff_rate_er       = np.vstack(( diff_rate_er, np.loadtxt(folder + 'diff_rate_er.txt')))\n",
    "        diff_rate_ac       = np.vstack(( diff_rate_ac, np.loadtxt(folder + 'diff_rate_ac.txt')))\n",
    "        diff_rate_cevns_SM = np.vstack(( diff_rate_cevns_SM, np.loadtxt(folder + 'diff_rate_CEVNS-SM.txt')))\n",
    "        diff_rate_radio    = np.vstack(( diff_rate_radio, np.loadtxt(folder + 'diff_rate_radiogenics.txt')))\n",
    "        diff_rate_wall     = np.vstack(( diff_rate_wall, np.loadtxt(folder + 'diff_rate_wall.txt')))\n",
    "        \n",
    "        s1s2_WIMP     = np.vstack((s1s2_WIMP, np.loadtxt(folder + 's1s2_WIMP.txt')))\n",
    "        s1s2_er       = np.vstack((s1s2_er, np.loadtxt(folder + 's1s2_er.txt')))\n",
    "        s1s2_ac       = np.vstack((s1s2_ac, np.loadtxt(folder + 's1s2_ac.txt')))\n",
    "        s1s2_cevns_SM = np.vstack((s1s2_cevns_SM, np.loadtxt(folder + 's1s2_CEVNS-SM.txt')))\n",
    "        s1s2_radio    = np.vstack((s1s2_radio, np.loadtxt(folder + 's1s2_radiogenics.txt')))\n",
    "        s1s2_wall     = np.vstack((s1s2_wall, np.loadtxt(folder + 's1s2_wall.txt')))\n",
    "        \n",
    "    \n",
    "nobs = len(pars) # Total number of observations\n",
    "print('We have ' + str(nobs) + ' observations...')\n",
    "\n",
    "diff_rate = diff_rate_WIMP + diff_rate_ac + diff_rate_cevns_SM + diff_rate_radio + diff_rate_wall + diff_rate_er \n",
    "\n",
    "s1s2 = s1s2_WIMP + s1s2_ac + s1s2_cevns_SM + s1s2_radio + s1s2_wall + s1s2_er\n",
    "rate = np.sum(s1s2, axis = 1) # Just to have the same as on the other notebooks. This already includes the backgrounds\n",
    "s1s2 = s1s2.reshape(nobs, 97, 97)\n",
    "\n",
    "# Let's work with the log of the mass and cross-section\n",
    "\n",
    "pars[:,0] = np.log10(pars[:,0])\n",
    "pars[:,1] = np.log10(pars[:,1])\n",
    "\n",
    "# Let's transform the diff_rate to counts per energy bin\n",
    "\n",
    "#diff_rate = np.round(diff_rate * 362440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rate_raw[np.where(rate_raw[:,3] == 0)[0],2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed584baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be always zero\n",
    "i = np.random.randint(nobs)\n",
    "print(rate_raw[i,2] - rate[i])\n",
    "print(rate_raw[i,2] - np.sum(diff_rate[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# shape of things #\n",
    "###################\n",
    "# we should get the same number of events in every file\n",
    "\n",
    "print(pars.shape)\n",
    "print(rate.shape)\n",
    "print(diff_rate.shape)\n",
    "\n",
    "# these are heavy guys:\n",
    "# signal:\n",
    "print(s1s2_WIMP.shape)\n",
    "# backgronds:\n",
    "print(s1s2_er.shape)\n",
    "print(s1s2_ac.shape)\n",
    "print(s1s2_cevns_SM.shape)\n",
    "print(s1s2_radio.shape)\n",
    "print(s1s2_wall.shape)\n",
    "\n",
    "###############\n",
    "# EXTRA FILES # backgrounds\n",
    "###############\n",
    "print(np.loadtxt(folder+'s1s2_CEVNS-NSI.txt').shape)\n",
    "print(np.loadtxt(folder+'s1s2_EVES-NSI.txt').shape)\n",
    "print(np.loadtxt(folder+'s1s2_EVES-SM.txt').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341088cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pars.shape)\n",
    "print(rate.shape)\n",
    "print(diff_rate.shape)\n",
    "print(s1s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_new = np.where(pars[:,1] < -36.5)[0]\n",
    "\n",
    "nobs = len(ind_new)\n",
    "pars = pars[ind_new]\n",
    "\n",
    "rate = rate[ind_new]\n",
    "rate_raw = rate_raw[ind_new]\n",
    "diff_rate = diff_rate[ind_new]\n",
    "s1s2 = s1s2[ind_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a484b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d758b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_new = np.random.choice(np.arange(nobs), nobs_new)\n",
    "\n",
    "nobs = len(ind_new)\n",
    "pars = pars[ind_new]\n",
    "\n",
    "rate = rate[ind_new]\n",
    "rate_raw = rate_raw[ind_new]\n",
    "diff_rate = diff_rate[ind_new]\n",
    "s1s2 = s1s2[ind_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rate[np.where(rate_raw[:,3] == 0)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe894792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split in training, validation and testing\n",
    "\n",
    "ntrain = int(70 * nobs / 100)\n",
    "nval   = int(25 * nobs / 100)\n",
    "ntest  = int(5 * nobs / 100)\n",
    "\n",
    "np.random.seed(28890)\n",
    "ind = np.random.choice(np.arange(nobs), size = nobs, replace = False)\n",
    "\n",
    "train_ind = ind[:ntrain]\n",
    "val_ind   = ind[ntrain:(ntrain + nval)]\n",
    "test_ind  = ind[(ntrain + nval):]\n",
    "\n",
    "pars_trainset = pars[train_ind,:]\n",
    "pars_valset   = pars[val_ind,:]\n",
    "pars_testset  = pars[test_ind,:]\n",
    "\n",
    "rate_trainset = rate[train_ind]\n",
    "rate_valset   = rate[val_ind]\n",
    "rate_testset  = rate[test_ind]\n",
    "\n",
    "diff_rate_trainset = diff_rate[train_ind,:]\n",
    "diff_rate_valset   = diff_rate[val_ind,:]\n",
    "diff_rate_testset  = diff_rate[test_ind,:]\n",
    "\n",
    "s1s2_trainset = s1s2[train_ind,:,:]\n",
    "s1s2_valset   = s1s2[val_ind,:,:]\n",
    "s1s2_testset  = s1s2[test_ind,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefc11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_min = np.min(pars_trainset, axis = 0)\n",
    "pars_max = np.max(pars_trainset, axis = 0)    \n",
    "\n",
    "x_rate = np.log10(rate_trainset) # Observable. Input data.\n",
    "x_min_rate = np.min(x_rate, axis = 0)\n",
    "x_max_rate = np.max(x_rate, axis = 0)\n",
    "\n",
    "x_drate = np.log10(diff_rate_trainset) # Observable. Input data. \n",
    "x_min_drate = np.min(x_drate, axis = 0)\n",
    "x_max_drate = np.max(x_drate, axis = 0)\n",
    "\n",
    "x_s1s2 = s1s2_trainset[:,:-1,:-1] # Observable. Input data. I am cutting a bit the images to have 64x64\n",
    "x_min_s1s2 = np.min(x_s1s2, axis = 0)\n",
    "x_max_s1s2 = np.max(x_s1s2).reshape(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97799ae",
   "metadata": {},
   "source": [
    "# Let's play with SWYFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bb972",
   "metadata": {},
   "source": [
    "## Using only the total rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5a5c2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rate = np.log10(rate_trainset) # Observable. Input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize everything between 0 and 1\n",
    "\n",
    "pars_norm = (pars_trainset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "x_norm_rate = (x_rate - x_min_rate) / (x_max_rate - x_min_rate)\n",
    "#x_norm_rate = x_rate / x_max_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55257731",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, gridspec_kw = {'hspace':0.5, 'wspace':0.5})\n",
    "\n",
    "ax[0,0].hist(x_norm_rate)\n",
    "ax[0,0].set_xlabel('# Events')\n",
    "\n",
    "ax[1,0].hist(pars_norm[:,0])\n",
    "ax[1,0].set_xlabel('$M_{DM}$')\n",
    "\n",
    "ax[0,1].hist(pars_norm[:,1])\n",
    "ax[0,1].set_xlabel('$\\sigma$')\n",
    "\n",
    "ax[1,1].hist(pars_norm[:,2])\n",
    "ax[1,1].set_xlabel('$\\\\theta$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad439b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_rate = x_norm_rate.reshape(len(x_norm_rate), 1)\n",
    "print(x_norm_rate.shape)\n",
    "print(pars_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca145bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_rate = swyft.Samples(x = x_norm_rate, z = pars_norm)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_rate = swyft.SwyftDataModule(samples_rate, fractions = [0.7, 0.25, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f884d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a network that estimates all the 1D and 2D marginal posteriors\n",
    "class Network_rate(swyft.SwyftModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        marginals = ((0, 1), (0, 2), (1, 2))\n",
    "        self.logratios1 = swyft.LogRatioEstimator_1dim(num_features = 1, num_params = 3, varnames = 'pars_norm')\n",
    "        self.logratios2 = swyft.LogRatioEstimator_Ndim(num_features = 1, marginals = marginals, varnames = 'pars_norm')\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        logratios1 = self.logratios1(A['x'], B['z'])\n",
    "        logratios2 = self.logratios2(A['x'], B['z'])\n",
    "        return logratios1, logratios2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffed8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricTracker(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.collection = []\n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, module):\n",
    "        elogs = trainer.logged_metrics # access it here\n",
    "        if 'train_loss' in elogs.keys():\n",
    "            self.val_loss.append(elogs['val_loss'])\n",
    "            self.train_loss.append(elogs['train_loss'])\n",
    "            self.collection.append(elogs)\n",
    "\n",
    "cb = MetricTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bceb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's configure, instantiate and traint the network\n",
    "torch.manual_seed(28890)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta = 0., patience=100, verbose=False, mode='min')\n",
    "checkpoint_callback     = ModelCheckpoint(monitor='val_loss', dirpath='./logs/', filename='O4_' + str(nobs_new) + '_final_rate_{epoch}_{val_loss:.2f}_{train_loss:.2f}', mode='min')\n",
    "trainer_rate = swyft.SwyftTrainer(accelerator = device, devices=1, max_epochs = 2000, precision = 64, callbacks=[early_stopping_callback, checkpoint_callback, cb])\n",
    "network_rate = Network_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dce7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rate = np.log10(rate_testset)\n",
    "x_norm_test_rate = (x_test_rate - x_min_rate) / (x_max_rate - x_min_rate)\n",
    "#x_norm_test_rate = x_test_rate  / x_max_rate\n",
    "x_norm_test_rate = x_norm_test_rate.reshape(len(x_norm_test_rate), 1)\n",
    "\n",
    "pars_norm_test = (pars_testset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_test_rate = swyft.Samples(x = x_norm_test_rate, z = pars_norm_test)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_test_rate = swyft.SwyftDataModule(samples_test_rate, fractions = [0., 0., 1], batch_size = 32)\n",
    "trainer_rate.test(network_rate, dm_test_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = True\n",
    "if fit:\n",
    "    trainer_rate.fit(network_rate, dm_rate)\n",
    "    checkpoint_callback.to_yaml('./logs/O4_' + str(nobs_new) + '_rate.yaml') \n",
    "    ckpt_path = swyft.best_from_yaml('./logs/O4_' + str(nobs_new) + '_rate.yaml')\n",
    "    #email('Termino de entrenar rate O4')\n",
    "    \n",
    "else:\n",
    "    ckpt_path = swyft.best_from_yaml('./logs/O4_' + str(nobs_new) + '_rate.yaml')\n",
    "\n",
    "# ---------------------------------------------- \n",
    "# It converges to val_loss = -1.18 at epoch ~50\n",
    "# ---------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5048ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rate = np.log10(rate_testset)\n",
    "x_norm_test_rate = (x_test_rate - x_min_rate) / (x_max_rate - x_min_rate)\n",
    "#x_norm_test_rate = x_test_rate / x_max_rate\n",
    "x_norm_test_rate = x_norm_test_rate.reshape(len(x_norm_test_rate), 1)\n",
    "pars_norm_test = (pars_testset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_test_rate = swyft.Samples(x = x_norm_test_rate, z = pars_norm_test)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_test_rate = swyft.SwyftDataModule(samples_test_rate, fractions = [0., 0., 1], batch_size = 32)\n",
    "trainer_rate.test(network_rate, dm_test_rate, ckpt_path = ckpt_path)\n",
    "\n",
    "# ---------------------------------------------- \n",
    "# It converges to val_loss = -1. in testset\n",
    "# ---------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f2ca7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "val_loss = []\n",
    "train_loss = []\n",
    "for i in range(1, len(cb.collection)):\n",
    "    train_loss.append( np.asarray(cb.train_loss[i].cpu()) )\n",
    "    val_loss.append( np.asarray(cb.val_loss[i].cpu()) )\n",
    "valLoss[0,iter] = np.min(val_loss)\n",
    "\n",
    "if False:\n",
    "\n",
    "    plt.plot(train_loss, label = 'Train Loss')\n",
    "    plt.plot(val_loss, label = 'Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.text(1.02,0.9,'Val Loss = ' + str(np.round(np.min(val_loss), 2)), transform = plt.gca().transAxes)\n",
    "    plt.text(1.02,0.95,'Train Loss = ' + str(np.round(np.min(train_loss), 2)), transform = plt.gca().transAxes)\n",
    "    plt.text(1.02,1,'Epoch = ' + str(np.argmin(train_loss)), transform = plt.gca().transAxes)\n",
    "    plt.legend()\n",
    "    plt.savefig('../graph/O4_' + str(nobs_new) + '_loss_rate.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afaa593",
   "metadata": {},
   "source": [
    "## Only using the total diff_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a57b5c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_drate = np.log10(diff_rate_trainset) # Observable. Input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21557b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize everything between 0 and 1\n",
    "\n",
    "\n",
    "pars_norm = (pars_trainset - pars_min) / (pars_max - pars_min)\n",
    "    \n",
    "x_norm_drate = (x_drate - x_min_drate) / (x_max_drate - x_min_drate)\n",
    "#x_norm_drate = x_drate / np.max(x_max_drate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, gridspec_kw = {'hspace':0.5, 'wspace':0.5})\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    ax[0,0].plot(x_norm_drate[i])\n",
    "ax[0,0].set_xlabel('$E_{r}$')\n",
    "\n",
    "ax[1,0].hist(pars_norm[:,0])\n",
    "ax[1,0].set_xlabel('$M_{DM}$')\n",
    "\n",
    "ax[0,1].hist(pars_norm[:,1])\n",
    "ax[0,1].set_xlabel('$\\sigma$')\n",
    "\n",
    "ax[1,1].hist(pars_norm[:,2])\n",
    "ax[1,1].set_xlabel('$\\\\theta$')\n",
    "\n",
    "ax[0,0].plot(x_norm_drate[502], c = 'black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_norm_drate.shape)\n",
    "print(pars_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f375bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_drate = swyft.Samples(x = x_norm_drate, z = pars_norm)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_drate = swyft.SwyftDataModule(samples_drate, fractions = [0.7, 0.25, 0.05], batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46963cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a network that estimates all the 1D and 2D marginal posteriors\n",
    "class Network(swyft.SwyftModule):\n",
    "    def __init__(self, lr = 1e-3, gamma = 1.):\n",
    "        super().__init__()\n",
    "        self.optimizer_init = swyft.OptimizerInit(torch.optim.Adam, dict(lr = lr, weight_decay=1e-5),\n",
    "              torch.optim.lr_scheduler.ExponentialLR, dict(gamma = gamma))\n",
    "        self.net = torch.nn.Sequential(\n",
    "          torch.nn.Linear(58, 500),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(500, 1000),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(1000, 500),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(500, 50),\n",
    "          torch.nn.ReLU(),\n",
    "          #torch.nn.Dropout(0.2),\n",
    "          torch.nn.Linear(50, 5)\n",
    "        )\n",
    "        marginals = ((0, 1), (0, 2), (1, 2))\n",
    "        self.logratios1 = swyft.LogRatioEstimator_1dim(num_features = 5, num_params = 3, varnames = 'pars_norm')\n",
    "        self.logratios2 = swyft.LogRatioEstimator_Ndim(num_features = 5, marginals = marginals, varnames = 'pars_norm')\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        img = torch.tensor(A['x'])\n",
    "        #z   = torch.tensor(B['z'])\n",
    "        f   = self.net(img)\n",
    "        logratios1 = self.logratios1(f, B['z'])\n",
    "        logratios2 = self.logratios2(f, B['z'])\n",
    "        return logratios1, logratios2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricTracker(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.collection = []\n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, module):\n",
    "        elogs = trainer.logged_metrics # access it here\n",
    "        if 'train_loss' in elogs.keys():\n",
    "            self.val_loss.append(elogs['val_loss'])\n",
    "            self.train_loss.append(elogs['train_loss'])\n",
    "            self.collection.append(elogs)\n",
    "\n",
    "cb = MetricTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197d12f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's configure, instantiate and traint the network\n",
    "torch.manual_seed(28890)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta = 0., patience=50, verbose=False, mode='min')\n",
    "checkpoint_callback     = ModelCheckpoint(monitor='val_loss', dirpath='./logs/', filename='O4_' + str(nobs_new) + '_drate_{epoch}_{val_loss:.2f}_{train_loss:.2f}', mode='min')\n",
    "trainer_drate = swyft.SwyftTrainer(accelerator = device, devices=1, max_epochs = 2000, precision = 64, callbacks=[early_stopping_callback, checkpoint_callback, cb])\n",
    "network_drate = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a03af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_drate = np.log10(diff_rate_testset)\n",
    "x_norm_test_drate = (x_test_drate - x_min_drate) / (x_max_drate - x_min_drate)\n",
    "#x_norm_test_drate = x_test_drate / np.max(x_max_drate)\n",
    "\n",
    "pars_norm_test = (pars_testset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_test_drate = swyft.Samples(x = x_norm_test_drate, z = pars_norm_test)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_test_drate = swyft.SwyftDataModule(samples_test_drate, fractions = [0., 0., 1], batch_size = 32)\n",
    "trainer_drate.test(network_drate, dm_test_drate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = True\n",
    "if fit:\n",
    "    trainer_drate.fit(network_drate, dm_drate)\n",
    "    checkpoint_callback.to_yaml(\"./logs/O4_\" + str(nobs_new) + \"_drate.yaml\") \n",
    "    ckpt_path = swyft.best_from_yaml(\"./logs/O4_\" + str(nobs_new) + \"_drate.yaml\")\n",
    "    #email('Termino el entramiento del drate para O4')\n",
    "else:\n",
    "    ckpt_path = swyft.best_from_yaml(\"./logs/O4_\" + str(nobs_new) + \"_drate.yaml\")\n",
    "\n",
    "# ---------------------------------------------- \n",
    "# It converges to val_loss = -1.8 @ epoch 20\n",
    "# ---------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04393dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_drate = np.log10(diff_rate_testset)\n",
    "x_norm_test_drate = (x_test_drate - x_min_drate) / (x_max_drate - x_min_drate)\n",
    "#x_norm_test_drate = x_test_drate / np.max(x_max_drate)\n",
    "\n",
    "pars_norm_test = (pars_testset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_test_drate = swyft.Samples(x = x_norm_test_drate, z = pars_norm_test)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_test_drate = swyft.SwyftDataModule(samples_test_drate, fractions = [0., 0., 1], batch_size = 32)\n",
    "trainer_drate.test(network_drate, dm_test_drate, ckpt_path = ckpt_path)\n",
    "\n",
    "# ---------------------------------------------- \n",
    "# It converges to -1.51 @ testset\n",
    "# ---------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3183eda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "val_loss = []\n",
    "train_loss = []\n",
    "for i in range(1, len(cb.collection)):\n",
    "    train_loss.append( np.asarray(cb.train_loss[i].cpu()) )\n",
    "    val_loss.append( np.asarray(cb.val_loss[i].cpu()) )\n",
    "\n",
    "valLoss[1,iter] = np.min(val_loss)\n",
    "\n",
    "if False:\n",
    "    \n",
    "    plt.plot(val_loss, label = 'Val Loss')\n",
    "    plt.plot(train_loss, label = 'Train Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.text(1.02,0.9,'Val Loss = ' + str(np.round(np.min(val_loss), 2)), transform = plt.gca().transAxes)\n",
    "    plt.text(1.02,0.95,'Train Loss = ' + str(np.round(np.min(train_loss), 2)), transform = plt.gca().transAxes)\n",
    "    plt.text(1.02,1,'Epoch = ' + str(np.argmin(train_loss)), transform = plt.gca().transAxes)\n",
    "    plt.legend()\n",
    "    plt.savefig('../graph/O4_' + str(nobs_new) + '_loss_drate.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afa736",
   "metadata": {},
   "source": [
    "## Using s1s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694b309",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19452b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s1s2 = s1s2_trainset[:,:-1,:-1] # Observable. Input data. I am cutting a bit the images to have 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9c931",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's normalize everything between 0 and 1\n",
    "\n",
    "pars_norm = (pars_trainset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "x_norm_s1s2 = x_s1s2\n",
    "#ind_nonzero = np.where(x_max_s1s2 > 0)\n",
    "#x_norm_s1s2[:,ind_nonzero[0], ind_nonzero[1]] = (x_s1s2[:,ind_nonzero[0], ind_nonzero[1]] - x_min_s1s2[ind_nonzero[0], ind_nonzero[1]]) / (x_max_s1s2[ind_nonzero[0], ind_nonzero[1]] - x_min_s1s2[ind_nonzero[0], ind_nonzero[1]])\n",
    "x_norm_s1s2 = x_s1s2 / x_max_s1s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d63bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, gridspec_kw = {'hspace':0.5, 'wspace':0.5})\n",
    "\n",
    "ax[0,0].hist(x_norm_s1s2[:,50,30])\n",
    "ax[0,0].set_xlabel('# Events')\n",
    "\n",
    "ax[1,0].hist(pars_norm[:,0])\n",
    "ax[1,0].set_xlabel('$M_{DM}$')\n",
    "\n",
    "ax[0,1].hist(pars_norm[:,1])\n",
    "ax[0,1].set_xlabel('$\\sigma$')\n",
    "\n",
    "ax[1,1].hist(pars_norm[:,2])\n",
    "ax[1,1].set_xlabel('$\\\\theta$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_s1s2 = x_norm_s1s2.reshape(len(x_norm_s1s2), 1, 96, 96) # The shape need to be (#obs, #channels, dim, dim)\n",
    "print(x_norm_s1s2.shape)\n",
    "print(pars_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42134fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_s1s2 = swyft.Samples(x = x_norm_s1s2, z = pars_norm)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_s1s2 = swyft.SwyftDataModule(samples_s1s2, fractions = [0.7, 0.25, 0.05], batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's define a network that estimates all the 1D and 2D marginal posteriors\n",
    "class Network(swyft.SwyftModule):\n",
    "    def __init__(self, lr = 1e-3, gamma = 1.):\n",
    "        super().__init__()\n",
    "        self.optimizer_init = swyft.OptimizerInit(torch.optim.Adam, dict(lr = lr, weight_decay=1e-5),\n",
    "              torch.optim.lr_scheduler.ExponentialLR, dict(gamma = gamma))\n",
    "        self.net = torch.nn.Sequential(\n",
    "          torch.nn.Conv2d(1, 10, kernel_size=5),\n",
    "          torch.nn.MaxPool2d(2),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Dropout(0.2),\n",
    "          torch.nn.Conv2d(10, 20, kernel_size=5, padding=2),\n",
    "          torch.nn.MaxPool2d(2),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Dropout(0.2),\n",
    "          torch.nn.Flatten(),\n",
    "          torch.nn.Linear(10580, 50),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Dropout(0.2),\n",
    "          torch.nn.Linear(50, 10),\n",
    "        )\n",
    "        marginals = ((0, 1), (0, 2), (1, 2))\n",
    "        self.logratios1 = swyft.LogRatioEstimator_1dim(num_features = 10, num_params = 3, varnames = 'pars_norm')\n",
    "        self.logratios2 = swyft.LogRatioEstimator_Ndim(num_features = 10, marginals = marginals, varnames = 'pars_norm')\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        img = torch.tensor(A['x'])\n",
    "        #z   = torch.tensor(B['z'])\n",
    "        f   = self.net(img)\n",
    "        logratios1 = self.logratios1(f, B['z'])\n",
    "        logratios2 = self.logratios2(f, B['z'])\n",
    "        return logratios1, logratios2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricTracker(Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.collection = []\n",
    "        self.val_loss = []\n",
    "        self.train_loss = []\n",
    "    \n",
    "    def on_validation_epoch_end(self, trainer, module):\n",
    "        elogs = trainer.logged_metrics # access it here\n",
    "        if 'train_loss' in elogs.keys():\n",
    "            self.val_loss.append(elogs['val_loss'])\n",
    "            self.train_loss.append(elogs['train_loss'])\n",
    "            self.collection.append(elogs)\n",
    "\n",
    "cb = MetricTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabca4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's configure, instantiate and traint the network\n",
    "torch.manual_seed(28890)\n",
    "cb = MetricTracker()\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta = 0., patience=50, verbose=False, mode='min')\n",
    "checkpoint_callback     = ModelCheckpoint(monitor='val_loss', dirpath='./logs/', filename='O4_' + str(nobs_new) + '_s1s2_{epoch}_{val_loss:.2f}_{train_loss:.2f}', mode='min')\n",
    "trainer_s1s2 = swyft.SwyftTrainer(accelerator = device, devices=1, max_epochs = 2500, precision = 64, callbacks=[early_stopping_callback, checkpoint_callback, cb])\n",
    "network_s1s2 = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae58c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_test_s1s2 = s1s2_testset[:,:-1,:-1] # Observable. Input data. I am cutting a bit the images to have 96x96\n",
    "x_norm_test_s1s2 = x_norm_test_s1s2 / x_max_s1s2 # Observable. Input data. I am cutting a bit the images to have 96x96\n",
    "x_norm_test_s1s2 = x_norm_test_s1s2.reshape(len(x_norm_test_s1s2), 1, 96, 96)\n",
    "\n",
    "pars_norm_test = (pars_testset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_test_s1s2 = swyft.Samples(x = x_norm_test_s1s2, z = pars_norm_test)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_test_s1s2 = swyft.SwyftDataModule(samples_test_s1s2, fractions = [0., 0., 1], batch_size = 32)\n",
    "trainer_s1s2.test(network_s1s2, dm_test_s1s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396acdc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fit = True\n",
    "if fit:\n",
    "    trainer_s1s2.fit(network_s1s2, dm_s1s2)\n",
    "    checkpoint_callback.to_yaml(\"./logs/O4_\" + str(nobs_new) + \"_s1s2.yaml\") \n",
    "    ckpt_path = swyft.best_from_yaml(\"./logs/O4_\" + str(nobs_new) + \"_s1s2.yaml\")\n",
    "    #email('Termino de entrenar s1s2 O4')\n",
    "    \n",
    "else:\n",
    "    ckpt_path = swyft.best_from_yaml(\"./logs/O4_\" + str(nobs_new) + \"_s1s2.yaml\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Min val loss value at 48 epochs. -3.31\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_s1s2.test(network_s1s2, dm_test_s1s2, ckpt_path = ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae554b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_test_s1s2 = s1s2_testset[:,:-1,:-1] # Observable. Input data. I am cutting a bit the images to have 96x96\n",
    "x_norm_test_s1s2 = x_norm_test_s1s2 / x_max_s1s2 # Observable. Input data. I am cutting a bit the images to have 96x96\n",
    "x_norm_test_s1s2 = x_norm_test_s1s2.reshape(len(x_norm_test_s1s2), 1, 96, 96)\n",
    "\n",
    "pars_norm_test = (pars_testset - pars_min) / (pars_max - pars_min)\n",
    "\n",
    "# We have to build a swyft.Samples object that will handle the data\n",
    "samples_test_s1s2 = swyft.Samples(x = x_norm_test_s1s2, z = pars_norm_test)\n",
    "\n",
    "# We have to build a swyft.SwyftDataModule object that will split the data into training, testing and validation sets\n",
    "dm_test_s1s2 = swyft.SwyftDataModule(samples_test_s1s2, fractions = [0., 0., 1], batch_size = 32)\n",
    "trainer_s1s2.test(network_s1s2, dm_test_s1s2, ckpt_path = ckpt_path)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Min val loss value at 7 epochs. -1.53 @ testset\n",
    "# ---------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f982018",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = []\n",
    "train_loss = []\n",
    "for i in range(1, len(cb.collection)):\n",
    "    train_loss.append( np.asarray(cb.train_loss[i].cpu()) )\n",
    "    val_loss.append( np.asarray(cb.val_loss[i].cpu()) )\n",
    "    \n",
    "valLoss[2,iter] = np.min(val_loss)\n",
    "\n",
    "if False:\n",
    "    \n",
    "    plt.plot(val_loss, label = 'Val Loss')\n",
    "    plt.plot(train_loss, label = 'Train Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.text(1.02,0.9,'Val Loss = ' + str(np.round(np.min(val_loss), 2)), transform = plt.gca().transAxes)\n",
    "    plt.text(1.02,0.95,'Train Loss = ' + str(np.round(np.min(train_loss), 2)), transform = plt.gca().transAxes)\n",
    "    plt.text(1.02,1,'Epoch = ' + str(np.argmin(train_loss)), transform = plt.gca().transAxes)\n",
    "    plt.savefig('../graph/O4_' + str(nobs_new) + '_loss_s1s2.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb44186",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435a0ed-22c4-4dc0-9c5e-672548c2fe56",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "np.save('O4_valLoss_nobs_' + str(nobs_new) + '.npy', valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba509e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30b02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
